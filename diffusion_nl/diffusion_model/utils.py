import os

import numpy as np
import pytorch_lightning as pl
import torch
import wandb

from minigrid.core.grid import Grid
from minigrid.core.constants import COLORS, IDX_TO_COLOR


def extract_agent_pos_and_direction(obs: np.array) -> tuple[int, int]:
    """
    Extracts the agent's position and direction from the observation

    Args:
        obs (np.array): environment observation
    """
    agent_pos = None
    agent_dir = None
    for i in range(len(obs)):
        for j in range(len(obs[i])):
            if obs[i][j][0] == 10:
                agent_pos = (i, j)
                agent_dir = obs[i][j][2]
                break
    return agent_pos, agent_dir


def state2img(state: np.array) -> np.array:
    """
    Take in an environment observation and convert it to an RGB image

    Args:
        state (np.array): environment observation

    Returns:
        img (np.array): RGB image of the environment observation
    """
    agent_pos, agent_dir = extract_agent_pos_and_direction(state)
    grid, _ = Grid.decode(state)

    # States generated by the diffusion model do not necessarily have an agent
    if agent_pos is not None:
        agent_type = state[agent_pos[0]][agent_pos[1]][1]
    else:
        agent_type = 0

    agent_color = COLORS[IDX_TO_COLOR[agent_type]]
    img = grid.render(32, agent_pos, agent_dir, agent_color=agent_color)
    return img


def transform_sample(sample: torch.Tensor):
    """
    Takes in a sample of states generated by the diffusion model and returns a list of RGB images

    Args:
        sample (torch.Tensor): sample of states generated by the diffusion model

    Returns:
        images (list[np.array]): list of RGB images
    """

    # Round to nearest integer
    sample = torch.round(sample)

    # Clamp values to valid range for each dimension
    sample[:, :, :, 0] = torch.clamp(sample[:, :, :, 0], 0, 10)
    sample[:, :, :, 1] = torch.clamp(sample[:, :, :, 1], 0, 5)
    sample[:, :, :, 2] = torch.clamp(sample[:, :, :, 2], 0, 4)

    sample = sample.cpu().numpy()

    # Convert environment observations to RGB images
    images = []
    for state in sample:
        img = state2img(state)
        images.append(img)

    return images


def get_eval_config(config: dict, action_space: int, model:pl.LightningModule, ivd_config: dict) -> dict:
    eval_config = {
        "action_space": action_space,
        "agent_type": config["evaluation"]["agent_type"],
        "num_distractors": config["env"]["num_distractors"],
        "use_agent_type": config["env"]["use_agent_type"],
        "env": "goto",
        "evaluation_episodes": config["evaluation"]["evaluation_episodes"],
        "num_envs": config["evaluation"]["num_envs"],
        "max_timesteps": config["evaluation"]["max_timesteps"],
        "model_store": config["logging"]["model_directory"],
        "dm_model_path": os.path.join(
            config["logging"]["experiment_name"], config["logging"]["project"]
        ),
        "dm_model_name": wandb.run.id,
        "planning_type": "ivd",
        "device": config["evaluation"]["device"],
        "visualize": False,
        "visualize_traj": False,
        "embeddings": {
            "model": config["data"]["embeddings"]["model"],
        },
        "n_example_frames": config["data"]["n_context_frames"],
        "model": model,
        "ivd_model_path": ivd_config[action_space]["ivd_model_path"],
        "ivd_model_name": ivd_config[action_space]["ivd_model_name"],
        "ivd_model_checkpoint": ivd_config[action_space]["ivd_model_checkpoint"],
        "example_path": config["data"]["example_path"],
    }
    return eval_config
